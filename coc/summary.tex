\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\graphicspath{{images/}}

\renewcommand{\familydefault}{\sfdefault}
\usepackage[a4paper]{geometry}

\usepackage{listings}

\usepackage{tcolorbox}
\newtcolorbox{keypointbox}
{
    arc=0mm,
    colback=red!20,
    colframe=red!80,
    leftrule=5pt,
    toprule=0pt,
    rightrule=0pt,
    bottomrule=0pt
}

\setcounter{secnumdepth}{2}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\fig}[4]{
	\begin{figure}[#1]
		\center
		\includegraphics[width=#2\textwidth]{#3.png}
		\caption{#4}
		\label{fig:#3}
	\end{figure}
	}

\usepackage{hyperref}
\usepackage{cleveref}

\title{Physically Based Simulation}
\author{Alexander Schl√∂gl}

\begin{document}
\maketitle

\tableofcontents

This is \textbf{my interpretation} of the lecture slides.
I tried to be very verbose and explain everything, all while removing irrelevant parts from the lecture.
Using this you should be able to pass the lecture easily.
\large{\textbf{However, I do not take responsibility for any bad results and will not be blamed from anyone.
This was a lot of work and I did it to save others (especially students of following semesters) from having to do this themselves.
Use this summary at your own responsibility.}}
If you have any feedback, feel free to create an issue on the \href{https://github.com/alxshine/lecture-notes}{git}.
I don't promise I will fix anything, but I will try.
\newpage

\section{Introduction to Compilers}
A compiler is a program that takes code written in a source language, which is usually a high-level language, and transforms it into a target language, often object code or machine code.
In the toolchain that transforms high level code to machine code, there also are other, compile-related programs, which may or may not work together with a compiler:
\begin{itemize}
	\item \textbf{Interpreters \& just-in-time compilers} often used for scripting languages (and Java)
	\item \textbf{Assemblers} translate the assembly language into machine code
	\item \textbf{Linkers} combine different object files into executable code
	\item \textbf{Loaders} load shared librabries (relocatable code)
	\item \textbf{Preprocessors} perform macro substitutions
	\item \textbf{Editors} are used to edit the code
	\item \textbf{Debuggers} allow step-by-step execution of the executable
	\item \textbf{Profilers} create memory and runtime profiles of the executable
	\item \textbf{Binary Inspection} allow inspection of the target code in the executable
\end{itemize}

\subsection{Language Description}
As a compiler needs to be tailored to the source and target language, describing languages is an essential part of building a compiler.
Languages are usually defined at three levels:
\begin{itemize}
	\item \textbf{Lexical level}:
		The lexical level of a language is defined by a dictionary.
		The dictionary contains a list of keywords and formats for the different data types, as well as valid variable names, usually defined using regular expressions.
	\item \textbf{Syntactical level}:
		The syntax of a language is defined by a grammar, describing valid control structures of the language.
	\item \textbf{Semantic level}:
		This describes the meaning of well-defined sentences in the language, and is often defined (in prose) in the language documentation.
\end{itemize}

\subsection{Phases of a Compiler}
A compiler operates in phases, split according to the tasks performed.
Common phases of a compiler are shown in \Cref{fig:phases}.
While the distinction between the phases is not always clear cut, keeping a degree of modularity is often beneficial.

\fig{h}{}{phases}{The phases of a compiler}

\subsubsection{Lexical Analysis (Scanning)}
Scanning is the process of taking in a stream of characters and outputting a stream of tokens.
This means splitting the source code into variable names, literals, etc.
During this phase a compiler can also enter \emph{identifiers} into the \emph{symbol table} and \emph{literals} into the \emph{literal table}.

\subsubsection{Syntactic Analysis (Parsing)}
During parsing the stream of tokens is used together with a grammar to create a \emph{syntax tree} and report syntax errors to the user.

\subsubsection{Semantic Analysis}
Semantic analysis checks the meaning of the program, and annotates the syntax tree with \emph{attributes}, e.g. declarations and data types.
Some semantics can only be checked while the programming is running (think dynamically allocated arrays), so not all errors are caught here.

\subsubsection{Intermediate Code Generation}
If multiple source and target languages or platforms are going to be supported it can be very beneficial to generate an intermediate representation that is independent of platform and language.
Using an intermediate representation removes the need of creating a compiler for every combination of source and target platform.
This reduces the number of parts that need to be written from $m*n$ to $m+n$, where $m$ is the number of source platforms and $n$ is the number of target platforms.
The required for adding a new source or target platform also drops from $m$ or $n$ to $1$.

Intermediate representations also have the benefit of making optimization through multiple passes easier.
A good example of intermediate representations being used is the LLVM compiler.

\subsubsection{Code Optimizer}
The code optimizer works on the intermediate representation by applying optimizations.
An optimization is a transformation that improves performance of the code in one or more metric.
Examples are dead code eleminition, consant folding or propagation, etc.

\subsubsection{Code Generator}
During this phase the actual target code is generated.
This can be Assembler, or any other target language.
Memory management, register allocation and instruction scheduling are the main challenges here.

\subsubsection{Target Code Optimizer}
In the last phase optimizations that are target specific are done.
This includes replacing instructions with faster ones, data pre-fetching and code parallelization where possible.

\subsection{T-Diagrams}
A compiler is defined by three languages:
\begin{itemize}
	\item \textbf{Host Language}: This is the language in which the compiler itself runs.
	\item \textbf{Source Language}: This is the language of the input.
	\item \textbf{Target Language}: This is the language the compiler produces.
\end{itemize}

Any and all of these three languages can be the same.
If a compiler produces code in a language that cannot be run on the host machine (the one doing the compilation), it is called a \emph{cross-compiler}.

Compilers are often represented using T-Diagrams, with the letters denoting the different languages.
An example is shown in \Cref{fig:bootstrapping}.

\subsubsection{Bootstrapping}
In order to create a compiler for a new language, one can save some work by employing a process called \emph{bootstrapping}.
During this process, a compiler for the new language is written in the new language, as it can then make use of the many neat features that were included in the new totally not bloated language that is super awesome and will end all other programming languages (/s).
The language creators then write a quick and dirty compiler in a different language.
This compiler doesn't have to be powerful, it only needs to be able to compile the "good" compiler.
By combining the two we then get a (hopefully) correct, but inefficient compiler.
Then we can recompile the "good" compiler with the minimal one to get the final version of the compiler, which can then compile all future versions of itself (until you include new language features).
The full workflow is shown in \Cref{fig:bootstrapping}

\fig{h}{}{bootstrapping}{The bootstrapping process in less text and more images}

\subsubsection{Porting}
Porting is the process of moving a compiler written it its own source language $A$ from machine $H$ to machine $K$.
In order to do this, a compiler is written in the source language $A$ with target language $K$, called a retargeted compiler.
This is then compiled with the original compiler and produces a cross-compiler.
The cross-compiler runs in language $H$ and produces language $K$ from source language $A$.
The retargeted compiler is then compiled with the cross compiler to create a compiler for language $A$ that runs in language $K$.
The entire workflow is shown in \Cref{fig:porting}

\fig{h}{0.9}{porting}{Porting a compiler}
\newpage

\subsubsection{Combining T-Diagrams}
Combining T-Diagrams is super easy and straight forward.
Just replace the language (or letter) to the left of the T-Diagram with the one on the right.
A few examples are shown in \Cref{fig:tcombining}

\fig{h}{0.8}{tcombining}{Combining T-Diagrams}

\section{Lexical Analysis (Scanning)}
During scanning we want to split a stream of characters into a stream of tokens.
The main goals are separating keywords, variables, constants, operators, etc.
We do this by creating regular expressions (regexes) for every type of token we want.
These regexes are then converted to finite automata, which we can simulate nicely.
We also want this phase of the compiler to be as efficient as possible, in order to save time for the more complex phases.

\subsection{Tokens}
Tokens in general are defined as an enumerated type, because you almost never define new keywords.
\begin{keypointbox}
	If you are using a language where you can define keywords dynamically, seek help.
\end{keypointbox}

Common token types are keywords (if, else), special symbols (+,-,*), identifiers (for variables) and numbers (int, float, ...).
These tokens can have one or more attributes, e.g. the string matched for the token (\emph{lexeme}), the numerical value for numbers, the operator for special symbols, ...
These attributes, as well as the type, are stored in a token record, which is just a simple container object or struct or whatever your language supports.
The scanner is usually controlled by the parser, which just calls it again and again, each time receiving a single token and processing it (adding it to the token table, matching grammar rules, ...).

As most of us are in computer science because we are both lazy and smart (being less dumb than the average is sufficient), we want to do as little work as possible, and because some people actually program for fun, we don't have to.
All we have to do is specify the different program tokens with regexes, and let an automatic tool build the lexical analyzer for us.

\subsection{Regular Expressions}
I will just skim regexes here because honestly, if you don't know them by now you have been cheating on all your CL exams, so why are you even reading this?
All you need to start is an alphabet $\Sigma$, and the rest works as defined by this very simple grammar:
\begin{equation*}
	E = a \in (E \cup \{\epsilon\})\ |\ E|E\ |\ EE\ |\ E^*\ |\ (E)
\end{equation*}
So regexes can do:
\begin{itemize}
	\item letters
	\item or
	\item concatenation
	\item repitition
	\item subexpressions
\end{itemize}
as defined in this order.
The notation for regexes is quite inconsistent, but this is the bare minimum, as defined in theoretical CS.
Most tools that use regexes can do some syntactic sugar, namely $+$ for one or more repititions, $.$ for any character, $\left[a-z\right]$ for any character (works with numbers as well), $\sim$ for set negation ($\hat{ }$ for character classes) and $?$ for optional subexpressions.
With this you can create most regexes quite comfortably.

\subsubsection{Ambiguity}
Some strings may be matched by several regular expressions.
Usually the longest match is used (e.g. "forever" is not a keyword, even though it starts with "for") but sometimes even that fails.
If there is doubt, most scanners will just use the rule that comes first, but for some more granular decisions token delimiters can be needed (e.g. line end or whitespace).
However, the problem with these is that they should not be consumed but instead returned to the input stream.
This is called a lookahead character.
Usually one lookahead character is enough, but sometimes more is required.

\subsection{Deterministic Finite Automata (DFA)}
Seriously, you should know this.
For an actual implementation, you just use a switch (or a series of ifs if you have to) for the incoming character and then go to the next state accordingly.
You can also do the same thing with a transition table, and code less, but transition tables can get very large.
Then you have to decide whether you want to implement sparse matrices (or use them if you can) or write a whole bunch of stupid switches.

\subsection{Constructing DFAs from Regexes}
This works just as it did in FLAT (or FML).
You create an NFA from the regexes (usually by combining NFAs for the subexpressions), and then create a DFA from that.
There's really not much to it, but if you're still unsure, just look at the FLAT slides, or ask you friends.


\section{Parsing}
We apparently didn't cover parsing in the lecture (honestly I wasn't there all too often so I don't know).
Accordingly, this chapter, as well as the sub-chapters top-down parsers and bottom-up parsers aren't part of the exam.
As I did write some stuff on top-down and bottom-up parsing before I learned this, I still have some information on them.
Including the whole sections was not sensible, but I did leave in the introductions to the subsections.
Anyway, here it goes:

Parsing is the process of converting the token stream into a syntax tree.
The syntax tree contains all syntactic blocks of the program (if, while, for, functions, ...), and in its structure contains their relations.
It is constructed according to rules defined in a context free grammar (I think context-sensitive grammars would also be possible, but the lecture slides only talk about context free grammars).
You should remember how CFGs work from FLAT, so I won't discuss them here (they were skipped in the lecture anyway).

The finished syntax tree is one of the most important artifacts of the compilation process.
After it is annotated with additional information (types, consant values, ...) it fully represents the program and its structure, and already allows some optimizations to take place.

The two main categories of parsing strategies are top-down and bottom-up.

\subsection{Top Down Parsing}
Top-down parsing starts with the first symbol of a token stream and follows the steps of a leftmost derivation of the grammar.
These parsers traverse the parse tree of the program in pre-order form from the root to the leaves.
As the next choice of derivation can sometimes be ambiguous, multiple methods of disambiguation exist.
One is using \emph{lookahead tokens}.
By looking ahead in the token stream far enough to find a point where the rules become disambiguous, the parser can always find the correct derivation.
This method causes some memory overhead, as the lookahead tokens have to be saved somewhere.

The alternative is using \emph{backtracking}, and just deriving according to any possible rule, and backtracking if the choice was incorrect.
This method potentially causes additional computation time due to the backtracking.
This computational overhead makes them too slow to be used for practical parsers.

\subsection{Bottom Up Parsing}
If you read the previous chapter, you know that choosing the right derivation step is hard for top down parsing.
This is the major strength of bottom-up parsers, because they can just take incoming tokens until a rule is fulfilled.
They are named and differentiated by their derivation policy and and the number of lookahead tokens, but in general they all follow the same procedure.
Their two main functions are \emph{shift}, which moves a token (or terminal) to the top of the stack, and \emph{reduce}, which replaces a string $\alpha$ on the stack with a nonterminal $A$ (according to a rule $A \rightarrow \alpha$).

\section{Attribute Grammars \& Semantic Analysis}
Attribute grammars are an extension to context free grammars.
They allow the inclusion of additional semantic information, like types and values, which are very useful for a compiler.
Attribute grammars are expressed through \emph{semantic rules} (also sometimes called \emph{attribute equations}).
Semantic analysis is usually much less formalized than syntactical analysis and still has to be handwritten everytime.
Semantic information is also very important for code optimization, which makes semantic analysis even more complicated and important.

The syntax of attribute grammars is $X.a$, where $a$ is an attribute associated with $X$.
It might be a data type, a value, a memory location or pretty much anything.
We differentiate between \emph{static attributes} which can be evaluated at compile time (like most types, and object code of a procedure or function) and \emph{dynamic attributes} which need to be evaluated at runtime (values of nonconstants and memory locations for dynamically allocated structures).
The time at which an attribute can be fixed is called the \emph{binding time} of that attribute.

An attribute equation or semantic rule is of the form
\begin{equation}
	X_i.a_j = f_{ij}(X_0.a_1, \dots, X_0.a_k, X_1.a_1, \dots, X_1.a_k, \dots, X_n, a_1, \dots, X_n.a_k)
\end{equation}
where all $X_i \in N \cup T$ (the union of nonterminals and terminals) are reachable by derivation rules from the start symbol, and $a_1, \dots, a_k$ are attributes.
In a less formal way this means the value of an attribute is a function of all other attributes.
For a concrete instance the arguments of $f$ are often only a subset of all attributes.
It is also important to note that the number of attribute equations for a semantic rule is not limited.
(However, as the form of $f$ is not really specified, one could argue that instead of using multiple attribute equations one could also use a more complex function).

For each grammar rule we can create a \emph{dependency graph}, giving us the order in which attributes need to be evaluated.
We can get the structure of this graph from the arguments of the individual $f$.
Depending on the dependency directions, we have different types of attributes: \emph{synthesized} and \emph{inherited}.
\end{document}
